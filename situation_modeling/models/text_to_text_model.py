import os
import torch
import itertools
import json
from pathlib import Path
from torch import nn,Tensor
import transformers
from torch.utils.data import DataLoader
from ..base import ConfigurableClass
from ..base_modules import (
    Text2TextModel,
    BuildText2Text
)    
from optparse import OptionParser,OptionGroup
from typing import List, Dict, Optional, Union, Tuple, Iterable,Type
from collections import OrderedDict
from .. import initialize_config
from dataclasses import dataclass
from tqdm.autonotebook import trange
from .model_base import (
    BasicAggregator,
    TranslationOutput
)    
from dataclasses import dataclass


class Text2TextSequenceGenerationModel(BasicAggregator):
    """A simple text2text model"""

    def __init__(self,model,global_config,device=None,):
        super().__init__()
        self.model = model
        self.global_config = global_config
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.logger.info("Use pytorch device: {}".format(device))

        self._target_device = torch.device(device)

    def forward(self, batch):
        ## forward, calls the underlying transformer model forward
        features,labels = batch
        return self.model(features,labels)

    def collate_fn(self,batch):
        """Main method for featurizing incoming batches 

        :param batch: the incoming batch 
        """
        return self.model.collate_fn(batch)

    @classmethod
    def from_config(cls,config):
        """Builds a classifier instance from config 

        :param config: the global configuration 
        """
        config.print_json = True

        ### build model
        #model = Text2TextModel.from_config(config)
        model = BuildText2Text(config)
        device = None if not config.device else config.device

        return cls(model,device=device,global_config=config)

    def evaluate_output(self,output,out_file=None):
        """Method for generating output produced during training and/or evaluation. 
        Passes by default. 

        :param output: the output generated by runner
        :raises: ValueError
        :returns: dictionary of metrics 
        :rtype: dict
        """
        ## single instance, for the on-the-fly querying 
        if isinstance(output,dict):
            return TranslationOutput.from_output(self.global_config,[output])

        ## evaluate dataset 
        self.logger.info('Evaluating model output')

        metrics = {}
        sout = TranslationOutput.from_output(self.global_config,output)

        if not self.global_config.skip_em:
            metrics["acc"] = sout.gen_em(
                sort=self.global_config.sort_output
            )

        if out_file:
            out_dir = Path(out_file).parent
            out_dir.mkdir(parents=True, exist_ok=True)
            with open(out_file,'w') as my_out:
                for instance in sout:
                    my_out.write(json.dumps(instance))
                    my_out.write("\n")

        return metrics

    def posthoc_analysis(self,config):
        return self.model.posthoc_analysis(config)
